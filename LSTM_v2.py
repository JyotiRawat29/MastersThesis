# -*- coding: utf-8 -*-
"""Untitled.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-62VTXwoyPL4V3QqsxwLk1L7gGAlnHgi
"""

import numpy as np
import torch
from torch import Tensor
import torch.nn.functional as f
from torch import nn
import scipy.io as sio
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import TensorDataset, DataLoader
from multiprocessing import cpu_count
import torch.nn.functional as F
import torch.optim as optim

from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter()

from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, chi2, f_classif

# Commented out IPython magic to ensure Python compatibility.
'''%load_ext tensorboard'
# %tensorboard â€” logdir logs'''

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from google.colab import drive
drive.mount('/content/drive')

"""# New Section"""

torch.cuda.empty_cache()
#load the ECG file
src_dataset = sio.loadmat('/content/drive/MyDrive/ECG2(withDA).mat')
#load the labels
label=pd.read_csv('/content/drive/MyDrive/label.csv',header=None)

#print(src_dataset)
testdata = src_dataset['ECG'] # use the key for data here
X=testdata['Data']

X=np.array(X[0])

X = np.vstack(X[:,]).astype(np.float)

#X = X[:,3350:4000]

X=torch.from_numpy(X)

X.dtype

label=np.array(label, dtype=object)
label=np.array(label)
label= np.vstack(label[:,]).astype(np.float)

df = pd.DataFrame(data =X)

df

label = pd.DataFrame(data = label)

label

df1 = pd.merge(df,label, left_index = True, right_index=True)

df1

df1.groupby('0_y').nunique()

a = df1.loc[df1['0_y']== 0,:]

b = df1.loc[df1['0_y']== 1,:]
c = df1.loc[df1['0_y']== 2,:]

print(a.shape)
print(b.shape)
print(c.shape)

#take 250 rows from each of the section
a = a[1:250]
b = b[1:250]
c = c[1:250]

#del a['0_y']
#del b['0_y']
#del c['0_y']

a = a.reset_index(drop = True)

b =b.reset_index(drop = True)
c = c.reset_index(drop = True)

df2 = pd.concat([a,b])

df2 = pd.concat([df2,c])

df2 = df2.reset_index(drop = True)

label_1 = df2['0_y']

del df2['0_y']

label_1 = label_1.to_numpy()

Y = torch.tensor(label_1)

Y.shape

Y = torch.tensor(label)

df2

X = df2.to_numpy()

print(X.shape)
print(Y.shape)

#from sklearn.datasets import make_classification

X,y = make_classification(
    n_features = 35,n_classes = 3, random_state = 42, n_informative=3, n_redundant =0, n_clusters_per_class=2
)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

anova_filter = SelectKBest(f_classif, k=3)

from sklearn.pipeline import make_pipeline
from sklearn.svm import LinearSVC

clf = LinearSVC()

anova_svm = make_pipeline(anova_filter, clf)
anova_svm.fit(X_train, y_train)

pca = PCA(n_components=4000, svd_solver= 'full')

pca.fit(X)

pca.explained_variance_ratio_

pca.singular_values_

X_pca = pca.singular_values_

X_pca = X_pca.reshape(1273,1)

X_new = SelectKBest(chi2, k='all').fit_transform(X_pca, label)

X_new.shape

X.shape

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify = Y)

X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size = 0.25, stratify = Y_train)

X_train.shape, X_valid.shape, Y_train.shape, Y_valid.shape

X_train.dtype

X_train.shape, X_valid.shape, Y_train.shape, Y_valid.shape

Y_valid.shape

Y_valid = torch.squeeze(Y_valid)
Y_valid.shape

Y_train = torch.squeeze(Y_train)
Y_test = torch.squeeze(Y_test)

#X_train = torch.tensor(X_train)
#X_valid = torch.tensor(X_valid)
#X_test = torch.tensor(X_test)

X_train, X_test, X_valid = [torch.tensor(arr) for arr in (X_train, X_test, X_valid )]

Y_train[1]

Y_train, Y_test, Y_valid = [torch.tensor(arr) for arr in (Y_train, Y_test, Y_valid)]

train_ds = TensorDataset(X_train, Y_train)

test_ds = TensorDataset(X_test, Y_test)
valid_ds = TensorDataset(X_valid, Y_valid)

train_dl = DataLoader(train_ds, batch_size = 32, shuffle = True)
test_dl = DataLoader(test_ds, batch_size = 32, shuffle = True)
valid_dl = DataLoader(valid_ds, batch_size=32, shuffle=True)

print(len(train_dl), len(test_dl), len(valid_dl))



def create_datasets(X, Y):
    enc = LabelEncoder()
    Y_enc = enc.fit_transform(Y)
    print("len of X and Y is",X.shape,Y.shape)
    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify = Y) 
    X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size=0.25, stratify = Y_train) 
    #squeeze labelled data to make it 1 dimension array
    Y_train=torch.squeeze(Y_train)
    Y_valid=torch.squeeze(Y_valid)
    Y_test=torch.squeeze(Y_test)
    print("X_train, X_valid, X_test shape before list comprehension",X_train.shape, X_valid.shape, X_test.shape)
    print("X_train, X_valid, X_test dtype before list comprehension",X_train.dtype, X_valid.dtype, X_test.dtype)
    print("Y_train, Y_valid, Y_test dtype before list comprehension",Y_train.dtype, Y_valid.dtype, Y_test.dtype)
    #the tensors created have dtype float64 but the models have dtypes float32 and its easier to change dtypes of the tensors instead of a model
    X_train, X_valid, X_test = [torch.tensor(arr, dtype=torch.float32) for arr in (X_train, X_valid,X_test)]
    Y_train, Y_valid,Y_test = [torch.tensor(arr, dtype=torch.long) for arr in (Y_train, Y_valid,Y_test)]
    print("X_train, X_valid, X_test shape after list comprehension",X_train.shape, X_valid.shape, X_test.shape)
    print("X_train, X_valid, X_test dtype before list comprehension",X_train.dtype, X_valid.dtype, X_test.dtype)
    print("Y_train, Y_valid, Y_test dtype before list comprehension",Y_train.dtype, Y_valid.dtype, Y_test.dtype)
    #convert training and testing data into Tensor Dataset
    train_ds = TensorDataset(X_train, Y_train)
    valid_ds = TensorDataset(X_valid, Y_valid)
    test_ds = TensorDataset(X_test,Y_test)
    print("train_ds, valid_ds, test_ds length of datasets respectively",len(train_ds), len(valid_ds), len(test_ds))
    return train_ds, valid_ds,test_ds, enc

def create_loaders(train_ds, valid_ds,test_ds, bs, jobs=0):
    train_dl = DataLoader(train_ds, bs, shuffle=True, num_workers=jobs)
    valid_dl = DataLoader(valid_ds, bs, shuffle=True, num_workers=jobs)
    test_dl = DataLoader(test_ds, bs, shuffle=True, num_workers=jobs)
    print("lenth of data loaders train_dl, valid_dl, test_dl",len(train_dl), len(valid_dl), len(test_dl))
    return train_dl, valid_dl, test_dl

def accuracy(output, target):
    return (output.argmax(dim=1) == target).float().mean().item()

class LSTMClassifier(nn.Module):
    """Very simple implementation of LSTM-based time-series classifier."""

    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.layer_dim = layer_dim
        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.batch_size = None
        self.hidden = None
        
    def forward(self, x):
        h0, c0 = self.init_hidden(x)
        print(x.size())
        out, (hn, cn) = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out


    def init_hidden(self, x):
        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)
        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)
        print(h0.shape)
        print(x.size(0))
        print(layer_dim)
        return [t.to(device) for t in (h0, c0)]

train_ds, val_ds, test_data, enc = create_datasets(X, Y)

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
bs =32
print(f'Creating data loaders with batch size: {bs}')
train_dl, val_dl,test_data = create_loaders(train_ds, val_ds,test_data, bs, jobs=cpu_count())

input_dim = 1
hidden_dim = 100
layer_dim = 1
output_dim = 1
seq_dim = 4000
lr = 0.01
n_epochs = 101
iterations_per_epoch = len(train_dl)
best_acc = 0
patience, trials = 100, 0

model = LSTMClassifier(input_dim,hidden_dim,layer_dim,output_dim)
model = model.to(device)
criteria = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr)

model

model.train()

#opt = torch.optim.Adam(model.parameters(), lr=0.001)
#criterion = nn.CrossEntropyLoss()

'''class LSTM(nn.Module):
    def __init__(self, input_size=1, hidden_layer_size=100, output_size=1):
        super().__init__()
        self.hidden_layer_size = hidden_layer_size

        self.lstm = nn.LSTM(input_size, hidden_layer_size,batch_first=True)

        self.linear = nn.Linear(hidden_layer_size, output_size)
        
        self.fc = nn.Linear(hidden_layer_size, output_size)

        self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size),
                            torch.zeros(1,1,self.hidden_layer_size))

    def forward(self, input_seq):
        h0 = torch.zeros(1, input_seq.size(0), self.hidden_layer_size).to(device) 
        c0 = torch.zeros(1, input_seq.size(0), self.hidden_layer_size).to(device)
        lstm_out, _ = self.lstm(input_seq, (h0,c0))
        lstm_out = self.fc(lstm_out[:, -1, :])
        #predictions = self.linear(lstm_out.view(len(input_seq), -1))
        #print("predictions",predictions)
        return lstm_out'''
    '''def init_hidden(self, x):
        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)
        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)
        
        print('hidden layer')
        print(h0.shape)
        print(x.size(0))
        print(layer_dim)
        return [t.to(device) for t in (h0, c0)]'''

'''trn_ds, val_ds, tst_data, enc = create_datasets(X, Y)
bs =50
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')'''

'''print(f'Creating data loaders with batch size: {bs}')
trn_dl, val_dl,tst_data = create_loaders(trn_ds, val_ds,tst_data, bs, jobs=cpu_count())
model = LSTM(1,500,1)
layer_dim = 1'''

'''model = model.to(device)'''

'''seq_dim = 4000
lr = 0.001
n_epochs = 101
iterations_per_epoch = len(trn_dl)
best_acc = 0
patience, trials = 100, 0
loss_function = nn.CrossEntropyLoss()
layer_dim = 1
output_dim = 1'''

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
from tensorflow import summary
# %load_ext tensorboard
train_log_dir ='logs/train'
test_log_dir = 'logs/test'
train_summary_writer = summary.create_file_writer(train_log_dir)
test_summary_writer = summary.create_file_writer(test_log_dir)



@tf.function 
def my_func(step,loss):
  with train_summary_writer.as_default():
    tf.summary.scalar("loss", loss.item(), step)

writer = SummaryWriter('runs')

tensorboard --logdir=runs

print('Start model training')
globaliter = 0

train_ds[0]

for epoch in range(1):
  for i, (x,y) in enumerate(train_ds):
    print("i is", i)
    print("x_batch, y_batch",x,y)
    print("x_batch", x.shape)
    print("x_ dimesns", x.ndim)
    print("y_batch", y.shape)
    print("y dimension", y.ndim)
  print("epoch is", epoch)
  print("train_dl length is",len(train_ds))

for i,(x_batch,y_batch) in enumerate(train_dl):
    if i in range(5):
      print("i is", i)
      print("x_batch, y_batch",x_batch,y_batch)
      print("x_batch shape", x_batch.shape)
      print("x_batch size", x_batch.size(0))
      print("x_batch", x_batch.ndim)
      print("y_batch", y_batch.shape)
      print("y_batch", y_batch.ndim)
      x_batch=torch.unsqueeze(x_batch,2)
      print("x_batch, y_batch",x_batch,y_batch)
      print("x_batch", x_batch.shape)
      print("x_batch", x_batch.ndim)
      print("y_batch", y_batch.shape)
      print("y_batch", y_batch.ndim)

    else:
      break

for i, (x_batch,y_batch) in enumerate(train_dl):
  if i in range(3):
    output = model(x_batch)
  else:
    break



for epoch in range(1, n_epochs + 1):

    for i, (x_batch, y_batch) in enumerate(train_dl):
        globaliter +=1
        #model.train() optional, it is done to set the model in training mode which is by default
        x_batch = x_batch.to(device)
        y_batch = y_batch.to(device)
        print('shape of the x_input batch', x_batch.shape)
        print('shape of the y_input batch', y_batch.shape)
        opt.zero_grad()
        #added the dimesnion here at 2nd position which makes the shape of tensor (32,4000,1)
        x_batch=torch.unsqueeze(x_batch,2)
        out = model(x_batch)
        y_batch=torch.unsqueeze(y_batch,0)
        print(y_batch.dtype)
        y_batch = y_batch.to(torch.float32)

        out = out.to(torch.float32)
        out=torch.transpose(out,1,0)
        loss = loss_function(out, torch.max(y_batch, 1)[1])
        loss.backward()
        opt.step()

    model.eval()
    correct, total = 0, 0
    #problem with the x_val size and type here
    for x_val, y_val in val_dl:


        x_val, y_val = [t for t in (x_val, y_val)]
        x_val = x_val.to(device)
        y_val = y_val.to(device)
        x_val=torch.unsqueeze(x_val,2)
        print(x_val.shape)
        out = model(x_val)
        preds = F.log_softmax(out, dim=1).argmax(dim=1)
        total += y_val.size(0)
        correct += (preds == y_val).sum().item()

    acc = correct / total

    if epoch % 5 == 0:
        print(f'Epoch: {epoch:3d}. Loss: {loss.item():.4f}. Acc.: {acc:2.2%}')

    if acc > best_acc:
        trials = 0
        best_acc = acc
        torch.save(model.state_dict(), 'best.pth')
        print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')
    else:
        trials += 1
        if trials >= patience:
            print(f'Early stopping on epoch {epoch}')
            break
    writer.add_scalars('Training vs validation Loss',
                       {'Training' : loss},
                       epoch *len(train_dl)+i)

writer.flush()

!tensorboard --logdir="/content/drive/MyDrive" --bind_all

model

model.load_state_dict(torch.load('best.pth'))
model.eval()

'''test_dl = DataLoader(tst_data, batch_size=64, shuffle=False)
test = []
print('Predicting on test dataset')'''

test_dl = DataLoader(tst_data, batch_size=64, shuffle=False)
test = []
print('Predicting on test dataset')
model = model.to(device)
for batch, _ in tst_data:
    batch=batch.to(device)
    batch = batch.reshape(4000,1,1)
    out = model.train()(batch)
    y_hat = F.log_softmax(out, dim=1).argmax(dim=1)
    test += y_hat.tolist()

test

y_hat

import tensorflow as tf

import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

a = tf.add(1,2)
b = tf.multiply(a,3)
c = tf.add(4,5)

d = tf.multiply(c,6)
e = tf.multiply(4,5)
f = tf.math.floordiv(c,6)

g = tf.add(b,d)
h = tf.multiply(g,f)
tf.compat.v1.disable_eager_execution()
with tf.compat.v1.Session() as sess:
  writer = tf.summary.FileWriter("./logs", sess.graph)
  print(sess.run(h))
  writer.close()

"""# New Section"""

